{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0110afd1-1df7-4137-a405-c29bf6278e6b",
   "metadata": {},
   "source": [
    "# Overview of the Gradient Boosted Tree Classifier\n",
    "\n",
    "Gradient Boosting is an ensemble learning method that takes many weak learners (with accuracy slightly above that of a random guess) and combines them sequentially to create a strong learner (model accuracy of >95%).\n",
    "\n",
    "Advantages:\n",
    "- Highly interpretable - the steps are intuitive\n",
    "- Versatile - can be used for both classification and regression problems\n",
    "\n",
    "Disadvantages:\n",
    "- If the hyperparameters aren't tuned carefully, it is easy for the model to overfit the training data\n",
    "- The model can become computationally expensive, as it requires subsequently training multiple weak learners\n",
    "\n",
    "### Representation\n",
    "\n",
    "We have chosen to implement Gradient Boosting for a classification problem, using a shallow decision tree as our weak learner. The final model $F(x)$ is therefore built from a sequence of $N$ trees, where each successive tree corrects the predictions from the previous iteration:\n",
    "\n",
    "$F_{N}(x) = F_{0}(x) + \\eta \\sum_{i=1}^{N} h_{i}(x)$ \n",
    "\n",
    "Where:\n",
    "- $F_{0}(x)$ is the initial prediction\n",
    "- $\\eta$ is the learning rate (a value between 0 and 1)\n",
    "- $h_{i}(x)$ is the prediction made by decision tree $i$, trained on the residuals from the previous trees\n",
    "\n",
    "We are not predicting class labels, but are instead predicting pseudo-residuals that will be used to correct the initial prediction.\n",
    "\n",
    "### Loss\n",
    "\n",
    "As this is a binary classification problem we will be using the Cross-Entropy loss/Log loss as our loss function. The function is:\n",
    "\n",
    "$L(y, \\hat{p}(x)) = -(y \\cdot log(\\hat{p}(x)) + (1-y) \\cdot log(1-\\hat{p}(x)))$\n",
    "\n",
    "Where:\n",
    "- $y$ are the true labels, 1 or 0\n",
    "- $\\hat{p}(x)$ is the probability predictor for the positive class, 1\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "The optimizer is gradient descent on the pseudo-residuals\n",
    "\n",
    "We calculate the pseudo-residuals as the negative gradient of the loss function with respect to the current model prediction:\n",
    "\n",
    "$r_i = -\\frac{\\partial L(y, \\hat{p}(x))}{\\partial F(x)} = y - \\hat{p}(x)$\n",
    "\n",
    "Shallow decision trees are trained on the previous iteration residuals, which are then used to update the prediction\n",
    "\n",
    "$F_{i+1}(x) = F_{i}(x) + \\eta \\cdot h_{i}(x)$ \n",
    "\n",
    "### Algorithm Pseudocode\n",
    "\n",
    "**inputs:**\n",
    "\n",
    "*training set:* $S = (x_1, y_1), ... , (x_m, y_m)$\n",
    "\n",
    "*weak learner: decision tree classifier* $DTC$\n",
    "\n",
    "*number of trees:* $N$\n",
    "\n",
    "*learning rate:* $\\eta$\n",
    "\n",
    "**initialize:**\n",
    "\n",
    "*set initial predictions as log-odds of the positive class:* $F_{0}(x) = logit(p_{y=1}) = log(\\frac{p_{y=1}}{1-p_{y=1}})$ \n",
    "\n",
    "**for** $i = 0, ...,N-1:$\n",
    "\n",
    "*compute the residuals:* $r_i = -\\frac{\\partial L(y, \\hat{p}_{i}(x))}{\\partial F_{i}(x)} = y - \\hat{p}_{i}(x)$\n",
    "\n",
    "*train a weak learner with residuals as targets:* $h_{i}(x) = DTC(F_{i}(x), S)$\n",
    "\n",
    "*update the model:* $F_{i+1}(x) = F_{i}(x) + \\eta \\cdot h_{i}(x)$\n",
    "\n",
    "**output:** \n",
    "\n",
    "*the predictions* $\\hat{y} = argmax(F_{N}(x))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b326a31-f613-4d67-b9d3-00cf2aa74b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
