{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0110afd1-1df7-4137-a405-c29bf6278e6b",
   "metadata": {},
   "source": [
    "# Overview of the Gradient Boosted Tree Classifier\n",
    "\n",
    "Gradient Boosting is an ensemble learning method that takes many weak learners (with accuracy slightly above that of a random guess) and combines them sequentially to create a strong learner (model accuracy of >95%).\n",
    "\n",
    "Advantages:\n",
    "- Highly interpretable - the steps are intuitive\n",
    "- Versatile - can be used for both classification and regression problems\n",
    "\n",
    "Disadvantages:\n",
    "- If the hyperparameters aren't tuned carefully, it is easy for the model to overfit the training data\n",
    "- The model can become computationally expensive, as it requires subsequently training multiple weak learners\n",
    "\n",
    "### Representation\n",
    "\n",
    "We have chosen to implement Gradient Boosting for a classification problem, using a shallow decision tree as our weak learner. The final model $F(x)$ is therefore built from a sequence of $N$ trees, where each successive tree corrects the predictions from the previous iteration:\n",
    "\n",
    "$F_{N}(x) = F_{0}(x) + \\eta \\sum_{i=1}^{N} h_{i}(x)$ \n",
    "\n",
    "Where:\n",
    "- $F_{0}(x)$ is the initial prediction\n",
    "- $\\eta$ is the learning rate (a value between 0 and 1)\n",
    "- $h_{i}(x)$ is the prediction made by decision tree $i$, trained on the residuals from the previous trees\n",
    "\n",
    "We are not predicting class labels, but are instead predicting pseudo-residuals that will be used to correct the initial prediction.\n",
    "\n",
    "### Loss\n",
    "\n",
    "As this is a binary classification problem we will be using the Cross-Entropy loss/Log loss as our loss function. The function is:\n",
    "\n",
    "$L(y, \\hat{p}(x)) = -(y \\cdot log(\\hat{p}(x)) + (1-y) \\cdot log(1-\\hat{p}(x)))$\n",
    "\n",
    "Where:\n",
    "- $y$ are the true labels, 1 or 0\n",
    "- $\\hat{p}(x)$ is the probability predictor for the positive class, 1\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "The optimizer is gradient descent on the pseudo-residuals\n",
    "\n",
    "We calculate the pseudo-residuals as the negative gradient of the loss function with respect to the current model prediction:\n",
    "\n",
    "$r_i = -\\frac{\\partial L(y, \\hat{p}(x))}{\\partial F(x)} = y - \\hat{p}(x)$\n",
    "\n",
    "Shallow decision trees are trained on the previous iteration residuals, which are then used to update the prediction\n",
    "\n",
    "$F_{i+1}(x) = F_{i}(x) + \\eta \\cdot h_{i}(x)$ \n",
    "\n",
    "### Algorithm Pseudocode\n",
    "\n",
    "**inputs:**\n",
    "\n",
    "*training set:* $S = (x_1, y_1), ... , (x_m, y_m)$\n",
    "\n",
    "*weak learner: decision tree classifier* $DTC$\n",
    "\n",
    "*number of trees:* $N$\n",
    "\n",
    "*learning rate:* $\\eta$\n",
    "\n",
    "**initialize:**\n",
    "\n",
    "*set initial predictions as log-odds of the positive class:* $F_{0}(x) = logit(p_{y=1}) = log(\\frac{p_{y=1}}{1-p_{y=1}})$ \n",
    "\n",
    "**for** $i = 0, ...,N-1:$\n",
    "\n",
    "*compute the residuals:* $r_i = -\\frac{\\partial L(y, \\hat{p}_{i}(x))}{\\partial F_{i}(x)} = y - \\hat{p}_{i}(x)$\n",
    "\n",
    "*train a weak learner with residuals as targets:* $h_{i}(x) = DTC(F_{i}(x), S)$\n",
    "\n",
    "*update the model:* $F_{i+1}(x) = F_{i}(x) + \\eta \\cdot h_{i}(x)$\n",
    "\n",
    "**output:** \n",
    "\n",
    "*the predictions* $\\hat{y} = argmax(F_{N}(x))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a2637-8438-445a-8925-359496904aa3",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fa765c1-b06d-4a46-85fa-61fe2cfeac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''\n",
    "        Sigmoid function f(x) =  1/(1 + exp(-x))\n",
    "        :param x: A scalar or Numpy array\n",
    "        :return: Sigmoid function evaluated at x (applied element-wise if it is an array)\n",
    "    '''\n",
    "    return np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (np.exp(x) + np.exp(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c370a-1abc-445c-849c-6fa76015e3d0",
   "metadata": {},
   "source": [
    "### Weak Learner: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a57cfae-68ef-47aa-b535-44b567bfcf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_score_entropy(prob): \n",
    "    # C(p) = -p * log(p) - (1-p) * log(1-p)\n",
    "    if prob >= 1:\n",
    "        score = 0\n",
    "    else:\n",
    "        if prob <= 0:\n",
    "            score = 0\n",
    "        else:\n",
    "            score = -prob*math.log(prob)-(1-prob)*math.log(1-prob)\n",
    "        \n",
    "    return score\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, left=None, right=None, depth=0, index_split_on=0, isleaf=False, label=1):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.depth = depth\n",
    "        self.index_split_on = index_split_on\n",
    "        self.isleaf = isleaf\n",
    "        self.label = label\n",
    "        self.info = {} # used for visualization\n",
    "\n",
    "\n",
    "    def _set_info(self, gain, num_samples):\n",
    "        self.info['gain'] = gain\n",
    "        self.info['num_samples'] = num_samples\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, data, validation_data=None, gain_function=node_score_entropy, max_depth=40):\n",
    "\n",
    "        # TODO: find majority class; set to class 0 if exactly balanced. This is the default label of your root node.\n",
    "        # Make sure to assign it to an instance variable `majority_class`.\n",
    "\n",
    "        y = [row[0] for row in data]  # Assuming the first column contains labels\n",
    "        class_counts = {0: y.count(0), 1: y.count(1)}\n",
    "        \n",
    "        # Set majority class (default label of the root node)\n",
    "        if class_counts[0] >= class_counts[1]:\n",
    "            self.majority_class = 0\n",
    "        else:\n",
    "            self.majority_class = 1\n",
    "            \n",
    "        self.max_depth = max_depth\n",
    "        self.root = Node(label = self.majority_class)\n",
    "        self.gain_function = gain_function\n",
    "\n",
    "        indices = list(range(1, len(data[0])))\n",
    "\n",
    "        self._split_recurs(self.root, data, indices)\n",
    "\n",
    "        # Pruning\n",
    "        if validation_data is not None:\n",
    "            self._prune_recurs(self.root, validation_data)\n",
    "\n",
    "\n",
    "    def predict(self, features):\n",
    "        '''\n",
    "        Helper function to predict the label given a row of features.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        return self._predict_recurs(self.root, features)\n",
    "\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        '''\n",
    "        Helper function to calculate the accuracy on the given data.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        return 1 - self.loss(data)\n",
    "\n",
    "\n",
    "    def loss(self, data):\n",
    "        '''\n",
    "        Helper function to calculate the loss on the given data.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        cnt = 0.0\n",
    "        test_Y = [row[0] for row in data]\n",
    "        for i in range(len(data)):\n",
    "            prediction = self.predict(data[i])\n",
    "            if (prediction != test_Y[i]):\n",
    "                cnt += 1.0\n",
    "        return cnt/len(data)\n",
    "\n",
    "\n",
    "    def _predict_recurs(self, node, row):\n",
    "        '''\n",
    "        Helper function to predict the label given a row of features.\n",
    "        Traverse the tree until leaves to get the label.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        if node.isleaf or node.index_split_on == 0:\n",
    "            return node.label\n",
    "        split_index = node.index_split_on\n",
    "        if not row[split_index]:\n",
    "            return self._predict_recurs(node.left, row)\n",
    "        else:\n",
    "            return self._predict_recurs(node.right, row)\n",
    "\n",
    "\n",
    "    def _prune_recurs(self, node, validation_data):\n",
    "        '''\n",
    "        TODO:\n",
    "        Prune the tree bottom up recursively. Nothing needs to be returned.\n",
    "        Do not prune if the node is a leaf.\n",
    "        Do not prune if the node is non-leaf and has at least one non-leaf child.\n",
    "        Prune if deleting the node could reduce loss on the validation data.\n",
    "        NOTE:\n",
    "        This might be slightly different from the pruning described in lecture.\n",
    "        Here we won't consider pruning a node's parent if we don't prune the node \n",
    "        itself (i.e. we will only prune nodes that have two leaves as children.)\n",
    "        HINT: Think about what variables need to be set when pruning a node!\n",
    "        '''\n",
    "        # Checks if node is not a Leaf\n",
    "        if not node.isleaf:\n",
    "            if node.left is not None:\n",
    "                self._prune_recurs(node.left, validation_data)\n",
    "                pass\n",
    "                \n",
    "            if node.right is not None:\n",
    "                self._prune_recurs(node.right, validation_data)\n",
    "                pass\n",
    "                \n",
    "            if (node.left.isleaf) and (node.right.isleaf):\n",
    "                # Calculate the loss before pruning\n",
    "                original_loss = self.loss(validation_data)\n",
    "                \n",
    "                # Temporarily prune this node (convert to leaf)\n",
    "                original_left, original_right = node.left, node.right\n",
    "                node.isleaf = True\n",
    "                node.left, node.right = None, None\n",
    "                \n",
    "                # Calculate the loss after pruning\n",
    "                pruned_loss = self.loss(validation_data)\n",
    "                \n",
    "                # If pruning doesn't reduce the loss, revert the pruning\n",
    "                if pruned_loss > original_loss:\n",
    "                    node.isleaf = False\n",
    "                    node.left, node.right = original_left, original_right\n",
    "\n",
    "        return\n",
    "                    \n",
    "        \n",
    "    def _is_terminal(self, node, data, indices):\n",
    "        '''\n",
    "        TODO:\n",
    "        Helper function to determine whether the node should stop splitting.\n",
    "        Stop the recursion if:\n",
    "            1. The dataset (as passed to parameter data) is empty.\n",
    "            2. There are no more indices to split on.\n",
    "            3. All the instances in this dataset belong to the same class\n",
    "            4. The depth of the node reaches the maximum depth.\n",
    "        Set the node label to be the majority label of the training dataset if:\n",
    "            1. The number of class 1 points is equal to the number of class 0 points.\n",
    "            2. The dataset is empty.\n",
    "        Return:\n",
    "            - A boolean, True indicating the current node should be a leaf and \n",
    "              False if the node is not a leaf.\n",
    "            - A label, indicating the label of the leaf (or the label the node would \n",
    "              be if we were to terminate at that node). If there is no data left, you\n",
    "              must return the majority class of the training set.\n",
    "        '''\n",
    "        y = [row[0] for row in data]        \n",
    "        \n",
    "        # TODO: Check Cases if the node should stop splitting\n",
    "        # TODO: Check cases if the node should be set to the majority label of the training dataset\n",
    "\n",
    "        node.isleaf = False\n",
    "        \n",
    "        if y.count(1)>y.count(0):\n",
    "            node.label = True\n",
    "        else:\n",
    "            if y.count(1)==y.count(0):\n",
    "                node.label = self.majority_class\n",
    "            else:\n",
    "                node.label = False\n",
    "                    \n",
    "        # the dataset is empty\n",
    "        if len(y) == 0:\n",
    "            node.isleaf = True\n",
    "            node.label = self.majority_class \n",
    "        \n",
    "        # there are no more indices to split on\n",
    "        if len(indices)==0:\n",
    "            node.isleaf = True\n",
    "\n",
    "        # all the instances in this dataset belong to the same class\n",
    "        if len(set(y))==1:\n",
    "            node.isleaf = True\n",
    "            if y.count(1)>0:\n",
    "                node.label = True\n",
    "            else:\n",
    "                node.label = False\n",
    "\n",
    "        # the depth of the node reaches the maximum depth\n",
    "        if (node.depth == self.max_depth):\n",
    "            node.isleaf = True\n",
    "        \n",
    "        return (node.isleaf, node.label)\n",
    "\n",
    "    def _split_recurs(self, node, data, indices):\n",
    "        '''\n",
    "        TODO:\n",
    "        Recursively split the node based on the rows and indices given.\n",
    "        Nothing needs to be returned.\n",
    "\n",
    "        First use _is_terminal() to check if the node needs to be split.\n",
    "        If so, select the column that has the maximum infomation gain to split on.\n",
    "        Store the label predicted for this node, the split column, and use _set_info()\n",
    "        to keep track of the gain and the number of datapoints at the split.\n",
    "        Then, split the data based on its value in the selected column.\n",
    "        The data should be recursively passed to the children.\n",
    "        '''\n",
    "        node.isleaf, node.label = self._is_terminal(node, data, indices)\n",
    "        \n",
    "        if not node.isleaf:\n",
    "            # TODO: Initialize and Calculate Gain\n",
    "            idx_max = 0\n",
    "            gain_max = 0\n",
    "            \n",
    "            for idx in indices:\n",
    "                gain = self._calc_gain(data, idx, self.gain_function)\n",
    "                if gain > gain_max:\n",
    "                    gain_max = gain\n",
    "                    idx_max = idx\n",
    "                \n",
    "            # TODO: Split the column and use _set_info() \n",
    "\n",
    "            if gain_max == 0:\n",
    "                node.isleaf = True\n",
    "                return\n",
    "\n",
    "            else:\n",
    "                split_index = idx_max\n",
    "                node.index_split_on = split_index\n",
    "                node._set_info(gain_max, len(data))\n",
    "                indices.remove(idx_max)\n",
    "                        \n",
    "            # TODO: Split the Data and Pass it recursively to the  children\n",
    "            best_left = [row for row in data if not row[split_index]]\n",
    "            best_right = [row for row in data if row[split_index]]\n",
    "            left_child = Node(depth=node.depth + 1, label=self.majority_class)\n",
    "            right_child = Node(depth=node.depth + 1, label=self.majority_class)\n",
    "            \n",
    "            node.left = left_child\n",
    "            node.right = right_child\n",
    "            \n",
    "            self._split_recurs(left_child, best_left, indices)\n",
    "            self._split_recurs(right_child, best_right, indices)\n",
    "            \n",
    "        return\n",
    "            \n",
    "    def _calc_gain(self, data, split_index, gain_function):\n",
    "        '''\n",
    "        TODO:\n",
    "        Calculate the gain of the proposed splitting and return it.\n",
    "        Gain = C(P[y=1]) - P[x_i=True] * C(P[y=1|x_i=True]) - P[x_i=False] * C(P[y=0|x_i=False])\n",
    "        Here the C(p) is the gain_function. For example, if C(p) = min(p, 1-p), this would be\n",
    "        considering training error gain. Other alternatives are entropy and gini functions.\n",
    "        '''\n",
    "        \n",
    "        y = [row[0] for row in data]\n",
    "        xi = [row[split_index] for row in data]\n",
    "        \n",
    "        if len(y) != 0 and len(xi) != 0:\n",
    "\n",
    "            prob_y = np.sum(y)/len(y)\n",
    "            prob_x = np.sum(xi)/len(xi)\n",
    "\n",
    "            y_true = np.array([])\n",
    "            y_false = np.array([])\n",
    "            \n",
    "            for j in range(len(y)):\n",
    "                if xi[j] == True:\n",
    "                   y_true = np.append(y_true,y[j]) \n",
    "                else:\n",
    "                   y_false = np.append(y_false, y[j]) \n",
    "\n",
    "            if len(y_true) != 0:\n",
    "                \n",
    "                prob_y_true = np.sum(y_true)/len(y_true)\n",
    "            else:\n",
    "                prob_y_true = 0\n",
    "\n",
    "            if len(y_false) !=0:\n",
    "                prob_y_false = 1-np.sum(y_false)/len(y_false)\n",
    "            else:\n",
    "                prob_y_false = 0\n",
    "                \n",
    "            gain = gain_function(prob_y)-prob_x*gain_function(prob_y_true)-(1-prob_x)*gain_function(prob_y_false) \n",
    "        else:\n",
    "            gain = 0\n",
    "\n",
    "        return gain\n",
    "    \n",
    "    def print_tree(self):\n",
    "        '''\n",
    "        Helper function for tree_visualization.\n",
    "        Only effective with very shallow trees.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        print('---START PRINT TREE---')\n",
    "        def print_subtree(node, indent=''):\n",
    "            if node is None:\n",
    "                return str(\"None\")\n",
    "            if node.isleaf:\n",
    "                return str(node.label)\n",
    "            else:\n",
    "                decision = 'split attribute = {:d}; gain = {:f}; number of samples = {:d}'.format(node.index_split_on, node.info['gain'], node.info['num_samples'])\n",
    "            left = indent + '0 -> '+ print_subtree(node.left, indent + '\\t\\t')\n",
    "            right = indent + '1 -> '+ print_subtree(node.right, indent + '\\t\\t')\n",
    "            return (decision + '\\n' + left + '\\n' + right)\n",
    "\n",
    "        print(print_subtree(self.root))\n",
    "        print('----END PRINT TREE---')\n",
    "\n",
    "\n",
    "    def loss_plot_vec(self, data):\n",
    "        '''\n",
    "        Helper function to visualize the loss when the tree expands.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        self._loss_plot_recurs(self.root, data, 0)\n",
    "        loss_vec = []\n",
    "        q = [self.root]\n",
    "        num_correct = 0\n",
    "        while len(q) > 0:\n",
    "            node = q.pop(0)\n",
    "            num_correct = num_correct + node.info['curr_num_correct']\n",
    "            loss_vec.append(num_correct)\n",
    "            if node.left != None:\n",
    "                q.append(node.left)\n",
    "            if node.right != None:\n",
    "                q.append(node.right)\n",
    "\n",
    "        return 1 - np.array(loss_vec)/len(data)\n",
    "\n",
    "\n",
    "    def _loss_plot_recurs(self, node, rows, prev_num_correct):\n",
    "        '''\n",
    "        Helper function to visualize the loss when the tree expands.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        labels = [row[0] for row in rows]\n",
    "        curr_num_correct = labels.count(node.label) - prev_num_correct\n",
    "        node.info['curr_num_correct'] = curr_num_correct\n",
    "\n",
    "        if not node.isleaf:\n",
    "            left_data, right_data = [], []\n",
    "            left_num_correct, right_num_correct = 0, 0\n",
    "            for row in rows:\n",
    "                if not row[node.index_split_on]:\n",
    "                    left_data.append(row)\n",
    "                else:\n",
    "                    right_data.append(row)\n",
    "\n",
    "            left_labels = [row[0] for row in left_data]\n",
    "            left_num_correct = left_labels.count(node.label)\n",
    "            right_labels = [row[0] for row in right_data]\n",
    "            right_num_correct = right_labels.count(node.label)\n",
    "\n",
    "            if node.left != None:\n",
    "                self._loss_plot_recurs(node.left, left_data, left_num_correct)\n",
    "            if node.right != None:\n",
    "                self._loss_plot_recurs(node.right, right_data, right_num_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec365e2-4574-450a-ae53-27d493847b7b",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "884883fe-0a0a-48f0-ae38-958a71b51b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDTC:\n",
    "    def __init__(self, n_estimators=2, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators=n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.tree_list = []\n",
    "        self.F_list = []\n",
    "        self.init_val = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # X: feature vectors 2D numpy array (row: # of data / column: # of features)\n",
    "        # y: label vectors (binary) \n",
    "\n",
    "        # Initial guess\n",
    "        self.init_val = np.log(np.mean(y) / (1 - np.mean(y)))\n",
    "        F = np.full(len(y), self.init_val)\n",
    "\n",
    "        # Iteration for the number of estimators\n",
    "        for i in range(self.n_estimators):\n",
    "            prob = sigmoid(F)\n",
    "            \n",
    "            residuals = y-prob\n",
    "            # Train a weak learner (decision tree) to the residuals\n",
    "            data_residuals = np.hstack((residuals.reshape(-1,1), X))\n",
    "            tree = DecisionTree(data_residuals, gain_function=node_score_entropy, max_depth=self.max_depth)\n",
    "            self.tree_list.append(tree)\n",
    "            \n",
    "            for i, x in enumerate(X):\n",
    "                pred = tree.predict(x) # predict the y(label) for each data\n",
    "                F[i] += self.learning_rate * pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        # X: feature vectors 2D numpy array (row: # of data / column: # of features)\n",
    "        F = np.full(X.shape[0], self.init_val)  # Start with the initial prediction\n",
    "        for tree in self.tree_list:\n",
    "            for i, x in enumerate(X):\n",
    "                pred = tree.predict(x) # predict the y(label) for each data of current tree\n",
    "                F[i] += self.learning_rate * pred\n",
    "\n",
    "        y_pred = (F>=0.5).astype(int)\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y):\n",
    "        # Calculate the score of the model with the dataset, corresponding to the GradientBoostingClassifier.score method\n",
    "        y_pred = self.predict(X)\n",
    "        mean_accuracy = (y_pred==y).sum()/len(y)\n",
    "\n",
    "        return mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "adb37b53-e05e-4068-8581-41ebdcdef482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([[1,0,0],[0,1,1],[1,0,1],[0,1,0],[1,1,1],[0,0,0]])\n",
    "y_train = np.array([0,1,1,0,0,0])\n",
    "\n",
    "X_test = np.array([[0,1,0],[1,1,1]])\n",
    "y_test = np.array([1,1])\n",
    "\n",
    "model = GBDTC(n_estimators=2, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "model.train(X_train,y_train)\n",
    "model.predict(X_test)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a9a9a-e1f7-4122-b1f9-92ab6636b14c",
   "metadata": {},
   "source": [
    "# Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "878bb373-b599-44e0-ab69-0952c6d36303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6553"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a scikit-learn GradientBoostingClassifier as an reference\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=6, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f413cac1-e733-4441-ba03-c60ad7b35b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for y_data\n",
    "y_train[y_train==-1] = 0\n",
    "y_test[y_test==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1cc08703-3b4b-4a80-840c-8244cb563307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.5049)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_compare = GBDTC(n_estimators=5, learning_rate=1.0, max_depth=1)\n",
    "clf_compare.train(X_train,y_train)\n",
    "clf_compare.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1302c7-a9de-449e-975a-5f63d97c2b29",
   "metadata": {},
   "source": [
    "### Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36715f87-433a-4000-ad68-1bb3f3b251c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1ada1d559a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    " \n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    " \n",
    "# Divide the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Create a CART template\n",
    "tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    " \n",
    "#Train the model on the training data\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions on test data\n",
    "y_pred = tree_classifier.predict(X_test)\n",
    " \n",
    "#Calculate the accuracy of the model<code>\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model accuracy:\", accuracy)\n",
    " \n",
    "#View the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=tree_classifier.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=tree_classifier.classes_)\n",
    "disp.plot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
