{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0110afd1-1df7-4137-a405-c29bf6278e6b",
   "metadata": {},
   "source": [
    "# Overview of the Gradient Boosted Tree Classifier\n",
    "\n",
    "Gradient Boosting is an ensemble learning method that takes many weak learners (with accuracy slightly above that of a random guess) and combines them sequentially to create a strong learner (model accuracy of >95%).\n",
    "\n",
    "Advantages:\n",
    "- Highly interpretable - the steps are intuitive\n",
    "- Versatile - can be used for both classification and regression problems\n",
    "\n",
    "Disadvantages:\n",
    "- If the hyperparameters aren't tuned carefully, it is easy for the model to overfit the training data\n",
    "- The model can become computationally expensive, as it requires subsequently training multiple weak learners\n",
    "\n",
    "### Representation\n",
    "\n",
    "We have chosen to implement Gradient Boosting for a classification problem, using a shallow decision tree as our weak learner. The final model $F(x)$ is therefore built from a sequence of $N$ trees, where each successive tree corrects the predictions from the previous iteration:\n",
    "\n",
    "$F_{N}(x) = F_{0}(x) + \\eta \\sum_{i=1}^{N} h_{i}(x)$ \n",
    "\n",
    "Where:\n",
    "- $F_{0}(x)$ is the initial prediction\n",
    "- $\\eta$ is the learning rate (a value between 0 and 1)\n",
    "- $h_{i}(x)$ is the prediction made by decision tree $i$, trained on the residuals from the previous trees\n",
    "\n",
    "We are not predicting class labels, but are instead predicting pseudo-residuals that will be used to correct the initial prediction.\n",
    "\n",
    "### Loss\n",
    "\n",
    "As this is a binary classification problem we will be using the Cross-Entropy loss/Log loss as our loss function. The function is:\n",
    "\n",
    "$L(y, \\hat{p}(x)) = -(y \\cdot log(\\hat{p}(x)) + (1-y) \\cdot log(1-\\hat{p}(x)))$\n",
    "\n",
    "Where:\n",
    "- $y$ are the true labels, 1 or 0\n",
    "- $\\hat{p}(x)$ is the probability predictor for the positive class, 1\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "The optimizer is gradient descent on the pseudo-residuals\n",
    "\n",
    "We calculate the pseudo-residuals as the negative gradient of the loss function with respect to the current model prediction:\n",
    "\n",
    "$r_i = -\\frac{\\partial L(y, \\hat{p}(x))}{\\partial F(x)} = y - \\hat{p}(x)$\n",
    "\n",
    "Shallow decision trees are trained on the previous iteration residuals, which are then used to update the prediction\n",
    "\n",
    "$F_{i+1}(x) = F_{i}(x) + \\eta \\cdot h_{i}(x)$ \n",
    "\n",
    "### Algorithm Pseudocode\n",
    "\n",
    "**inputs:**\n",
    "\n",
    "&emsp; *training set:* $S = (x_1, y_1), ... , (x_m, y_m)$\n",
    "\n",
    "&emsp; *weak learner: decision tree classifier* $DTC$\n",
    "\n",
    "&emsp; *number of trees:* $N$\n",
    "\n",
    "&emsp; *learning rate:* $\\eta$\n",
    "\n",
    "**initialize:**\n",
    "\n",
    "&emsp; *set initial predictions as log-odds of the positive class:* $F_{0}(x) = logit(p_{y=1}) = log(\\frac{p_{y=1}}{1-p_{y=1}})$ \n",
    "\n",
    "&emsp; **for** $i = 0, ...,N-1:$\n",
    "\n",
    "&emsp;&emsp; *compute the residuals:* $r_i = -\\frac{\\partial L(y, \\hat{p}_{i}(x))}{\\partial F_{i}(x)} = y - \\hat{p}_{i}(x)$\n",
    "\n",
    "&emsp;&emsp; *train a weak learner with residuals as targets:* $h_{i}(x) = DTC(F_{i}(x), S)$\n",
    "\n",
    "&emsp;&emsp; *update the model:* $F_{i+1}(x) = F_{i}(x) + \\eta \\cdot h_{i}(x)$\n",
    "\n",
    "**output:** \n",
    "\n",
    "&emsp; *the predictions* $\\hat{y} = argmax(F_{N}(x))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a2637-8438-445a-8925-359496904aa3",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c370a-1abc-445c-849c-6fa76015e3d0",
   "metadata": {},
   "source": [
    "### Weak Learner: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a57cfae-68ef-47aa-b535-44b567bfcf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement decision tree here - use HW solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec365e2-4574-450a-ae53-27d493847b7b",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "884883fe-0a0a-48f0-ae38-958a71b51b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement gradient boosting following the pseudocode outlined in the overview section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a9a9a-e1f7-4122-b1f9-92ab6636b14c",
   "metadata": {},
   "source": [
    "# Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c13fe2-7373-4aca-a33e-39547d085be1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
