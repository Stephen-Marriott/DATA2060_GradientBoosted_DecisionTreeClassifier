{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0110afd1-1df7-4137-a405-c29bf6278e6b",
   "metadata": {},
   "source": [
    "# Overview of the Gradient Boosted Tree Classifier\n",
    "\n",
    "Gradient Boosting is an ensemble learning method that takes many weak learners (with accuracy slightly above that of a random guess) and combines them sequentially to create a strong learner (model accuracy of >95%).\n",
    "\n",
    "Advantages:\n",
    "- Highly interpretable - the steps are intuitive\n",
    "- Versatile - can be used for both classification and regression problems\n",
    "\n",
    "Disadvantages:\n",
    "- If the hyperparameters aren't tuned carefully, it is easy for the model to overfit the training data\n",
    "- The model can become computationally expensive, as it requires subsequently training multiple weak learners\n",
    "\n",
    "### Representation\n",
    "\n",
    "We have chosen to implement Gradient Boosting for a classification problem, using a shallow decision tree as our weak learner. The final model $F(x)$ is therefore built from a sequence of $N$ trees, where each successive tree corrects the predictions from the previous iteration:\n",
    "\n",
    "$F_{N}(x) = F_{0}(x) + \\eta \\sum_{i=1}^{N} h_{i}(x)$ \n",
    "\n",
    "Where:\n",
    "- $F_{0}(x)$ is the initial prediction\n",
    "- $\\eta$ is the learning rate (a value between 0 and 1)\n",
    "- $h_{i}(x)$ is the prediction made by decision tree $i$, trained on the residuals from the previous trees\n",
    "\n",
    "We are not predicting class labels, but are instead predicting pseudo-residuals that will be used to correct the initial prediction.\n",
    "\n",
    "### Loss\n",
    "\n",
    "As this is a binary classification problem we will be using the Cross-Entropy loss/Log loss as our loss function. The function is:\n",
    "\n",
    "$L(y, \\hat{p}(x)) = -(y \\cdot log(\\hat{p}(x)) + (1-y) \\cdot log(1-\\hat{p}(x)))$\n",
    "\n",
    "Where:\n",
    "- $y$ are the true labels, 1 or 0\n",
    "- $\\hat{p}(x)$ is the probability predictor for the positive class, 1\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "The optimizer is gradient descent on the pseudo-residuals\n",
    "\n",
    "We calculate the pseudo-residuals as the negative gradient of the loss function with respect to the current model prediction:\n",
    "\n",
    "$r_i = -\\frac{\\partial L(y, \\hat{p}(x))}{\\partial F(x)} = y - \\hat{p}(x)$\n",
    "\n",
    "Shallow decision trees are trained on the previous iteration residuals, which are then used to update the prediction\n",
    "\n",
    "$F_{i+1}(x) = F_{i}(x) + \\eta \\cdot h_{i}(x)$ \n",
    "\n",
    "### Algorithm Pseudocode\n",
    "\n",
    "**inputs:**\n",
    "\n",
    "*training set:* $S = (x_1, y_1), ... , (x_m, y_m)$\n",
    "\n",
    "*weak learner: decision tree classifier* $DTC$\n",
    "\n",
    "*number of trees:* $N$\n",
    "\n",
    "*learning rate:* $\\eta$\n",
    "\n",
    "**initialize:**\n",
    "\n",
    "*set initial predictions as log-odds of the positive class:* $F_{0}(x) = logit(p_{y=1}) = log(\\frac{p_{y=1}}{1-p_{y=1}})$ \n",
    "\n",
    "**for** $i = 0, ...,N-1:$\n",
    "\n",
    "*compute the residuals:* $r_i = -\\frac{\\partial L(y, \\hat{p}_{i}(x))}{\\partial F_{i}(x)} = y - \\hat{p}_{i}(x)$\n",
    "\n",
    "*train a weak learner with residuals as targets:* $h_{i}(x) = DTC(F_{i}(x), S)$\n",
    "\n",
    "*update the model:* $F_{i+1}(x) = F_{i}(x) + \\eta \\cdot h_{i}(x)$\n",
    "\n",
    "**output:** \n",
    "\n",
    "*the predictions* $\\hat{y} = argmax(F_{N}(x))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a2637-8438-445a-8925-359496904aa3",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3fa765c1-b06d-4a46-85fa-61fe2cfeac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''\n",
    "        Sigmoid function f(x) =  1/(1 + exp(-x))\n",
    "        :param x: A scalar or Numpy array\n",
    "        :return: Sigmoid function evaluated at x (applied element-wise if it is an array)\n",
    "    '''\n",
    "    return np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (np.exp(x) + np.exp(0)))\n",
    "\n",
    "\n",
    "def cal_mse(y):\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    \n",
    "    mean = np.mean(y)\n",
    "\n",
    "    return np.mean((y - mean) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c370a-1abc-445c-849c-6fa76015e3d0",
   "metadata": {},
   "source": [
    "### Weak Learner: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57cfae-68ef-47aa-b535-44b567bfcf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, left=None, right=None, depth=0, index_split_on=0, isleaf=False, threshold=None, value=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.depth = depth\n",
    "        self.index_split_on = index_split_on\n",
    "        self.isleaf = isleaf\n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, data, max_depth=40):\n",
    "\n",
    "        # label data is contained in the first column of data\n",
    "        y = [row[0] for row in data]\n",
    "        \n",
    "        self.average_value = np.mean(y)\n",
    "        self.max_depth = max_depth\n",
    "        self.root = Node(value=self.average_value)\n",
    "        indices = list(range(1, len(data[0])))\n",
    "\n",
    "        self._split_recurs(self.root, data, indices)\n",
    "\n",
    "    def predict(self, features):\n",
    "\n",
    "        return self._predict_recurs(self.root, features)\n",
    "\n",
    "\n",
    "    def _predict_recurs(self, node, row):\n",
    "\n",
    "        if node.isleaf or node.index_split_on == 0:\n",
    "            return node.value\n",
    "\n",
    "        if row[node.index_split_on-1] <= node.threshold:\n",
    "            return self._predict_recurs(node.left, row)\n",
    "        \n",
    "        else:\n",
    "            return self._predict_recurs(node.right, row)\n",
    "\n",
    "    def _is_terminal(self, node, data, indices):\n",
    "\n",
    "        y = np.array([row[0] for row in data])        \n",
    "                  \n",
    "        # the dataset is empty\n",
    "        if len(y) == 0:\n",
    "            node.isleaf = True\n",
    "            node.value = None\n",
    "\n",
    "        else:\n",
    "            node.isleaf = False\n",
    "            node.value = np.mean(y)\n",
    "        \n",
    "        # there are no more indices to split on\n",
    "        # all the instances in this dataset belong to the same class\n",
    "        # the depth of the node reaches the maximum depth\n",
    "        if (len(indices)==0) or (len(set(y))==1) or (node.depth == self.max_depth):\n",
    "            node.isleaf = True\n",
    "        \n",
    "        return (node.isleaf, node.value)\n",
    "\n",
    "\n",
    "    def _split_recurs(self, node, data, indices):\n",
    "\n",
    "        node.isleaf, node.value = self._is_terminal(node, data, indices)\n",
    "\n",
    "        if not node.isleaf:\n",
    "\n",
    "            best_idx = None\n",
    "            best_mse = float('inf')\n",
    "            best_threshold = None\n",
    "            \n",
    "            data= np.array(data)\n",
    "            y = data[:,0]\n",
    "            \n",
    "            # find the best split index\n",
    "            for idx in indices:\n",
    "                thresholds = np.unique(data[:, idx])\n",
    "\n",
    "                # find the threshold for each split index\n",
    "                for threshold in thresholds:\n",
    "                     \n",
    "                    left_idx = data[:, idx] <= threshold\n",
    "                    right_idx = ~left_idx\n",
    "                    left = data[left_idx,:]\n",
    "                    right = data[right_idx,:]\n",
    "                    mse = cal_mse(y[left_idx]) + cal_mse(y[right_idx])\n",
    "\n",
    "                    if mse < best_mse:\n",
    "                        best_idx = idx\n",
    "                        best_threshold = threshold \n",
    "                        best_mse = mse\n",
    "\n",
    "            if best_idx == None:\n",
    "                node.isleaf = True\n",
    "                node.value = np.mean(y)\n",
    "                return\n",
    "\n",
    "            else:\n",
    "                node.threshold = best_threshold\n",
    "                node.index_split_on = best_idx\n",
    "                indices.remove(best_idx)\n",
    "                \n",
    "            # TODO: Split the Data and Pass it recursively to the  children\n",
    "            \n",
    "            best_left = [row for row in data if row[best_idx]<=best_threshold]\n",
    "            best_right = [row for row in data if row[best_idx]>best_threshold]\n",
    "            left_child = Node(depth=node.depth + 1)\n",
    "            right_child = Node(depth=node.depth + 1)\n",
    "            \n",
    "            node.left = left_child\n",
    "            node.right = right_child\n",
    "            \n",
    "            self._split_recurs(left_child, best_left, indices)\n",
    "            self._split_recurs(right_child, best_right, indices)\n",
    "            \n",
    "        return\n",
    "         \n",
    "\n",
    "    def loss(self, data):\n",
    "        y_pred = np.zeros((len(data),1))\n",
    "        y = np.array(data)[:,0].reshape(-1,1)\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            y_pred[i] = self.predict(data[i,1:])\n",
    "        loss = np.mean((y - y_pred) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec365e2-4574-450a-ae53-27d493847b7b",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "884883fe-0a0a-48f0-ae38-958a71b51b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDTC:\n",
    "    def __init__(self, n_estimators=2, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators=n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.tree_list = []\n",
    "        self.F_list = []\n",
    "        self.init_val = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # X: feature vectors 2D numpy array (row: # of data / column: # of features)\n",
    "        # y: label vectors (binary) \n",
    "\n",
    "        # Initial guess\n",
    "        self.init_val = np.log(np.mean(y) / (1 - np.mean(y)))\n",
    "        F = np.full(len(y), self.init_val)\n",
    "\n",
    "        # Iteration for the number of estimators\n",
    "        for i in range(self.n_estimators):\n",
    "            prob = sigmoid(F)\n",
    "            \n",
    "            residuals = y-prob\n",
    "            # Train a weak learner (decision tree) to the residuals\n",
    "            data_residuals = np.hstack((residuals.reshape(-1,1), X))\n",
    "            tree = DecisionTree(data_residuals, max_depth=self.max_depth)\n",
    "            self.tree_list.append(tree)\n",
    "            \n",
    "            for i, x in enumerate(X):\n",
    "                pred = tree.predict(x) # predict the y(label) for each data\n",
    "                F[i] += self.learning_rate * pred\n",
    "\n",
    "    def predict(self, X):\n",
    "        # X: feature vectors 2D numpy array (row: # of data / column: # of features)\n",
    "        F = np.full(X.shape[0], self.init_val)  # Start with the initial prediction\n",
    "        for tree in self.tree_list:\n",
    "            for i, x in enumerate(X):\n",
    "                pred = tree.predict(x) # predict the y(label) for each data of current tree\n",
    "                F[i] += self.learning_rate * pred\n",
    "\n",
    "        y_pred = (F>=0.5).astype(int)\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y):\n",
    "        # Calculate the score of the model with the dataset, corresponding to the GradientBoostingClassifier.score method\n",
    "        y_pred = self.predict(X)\n",
    "        mean_accuracy = (y_pred==y).sum()/len(y)\n",
    "\n",
    "        return mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "adb37b53-e05e-4068-8581-41ebdcdef482",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[277], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m GBDTC(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mscore(X_test,y_test)\n",
      "Cell \u001b[1;32mIn[276], line 29\u001b[0m, in \u001b[0;36mGBDTC.train\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_list\u001b[38;5;241m.\u001b[39mappend(tree)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(X):\n\u001b[1;32m---> 29\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# predict the y(label) for each data\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     F[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m pred\n",
      "Cell \u001b[1;32mIn[256], line 30\u001b[0m, in \u001b[0;36mDecisionTree.predict\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, features):\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    Helper function to predict the label given a row of features.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    You do not need to modify this.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_recurs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[256], line 42\u001b[0m, in \u001b[0;36mDecisionTree._predict_recurs\u001b[1;34m(self, node, row)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39misleaf \u001b[38;5;129;01mor\u001b[39;00m node\u001b[38;5;241m.\u001b[39mindex_split_on \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_split_on\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mthreshold:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_recurs(node\u001b[38;5;241m.\u001b[39mleft, row)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[1,0,0],[0,1,1],[1,0,1],[0,1,0],[1,1,1],[0,0,0]])\n",
    "y_train = np.array([0,1,1,0,0,0])\n",
    "\n",
    "X_test = np.array([[0,1,0],[1,1,1]])\n",
    "y_test = np.array([1,1])\n",
    "\n",
    "model = GBDTC(n_estimators=2, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "model.train(X_train,y_train)\n",
    "model.predict(X_test)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db697af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "         \n",
    "class Node:\n",
    "    def __init__(self, depth, max_depth=None):\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "# Mean Squared Error (MSE) as the impurity measure\n",
    "def calculate_mse(y):\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    mean = np.mean(y)\n",
    "    return np.mean((y - mean) ** 2)\n",
    "\n",
    "# Split the data based on a given feature and threshold\n",
    "def split_data(X, y, feature_index, threshold):\n",
    "    left_mask = X[:, feature_index] <= threshold\n",
    "    right_mask = ~left_mask\n",
    "    return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "# Find the best split for a node\n",
    "def find_best_split(X, y):\n",
    "    m, n = X.shape\n",
    "    if m <= 1:\n",
    "        return None, None\n",
    "\n",
    "    best_mse = float('inf')\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "\n",
    "    for feature_index in range(n):\n",
    "        thresholds = np.unique(X[:, feature_index])\n",
    "        for threshold in thresholds:\n",
    "            X_left, X_right, y_left, y_right = split_data(X, y, feature_index, threshold)\n",
    "            mse = calculate_mse(y_left) + calculate_mse(y_right)\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_feature = feature_index\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_feature, best_threshold\n",
    "\n",
    "\n",
    "# Build the decision tree recursively\n",
    "def build_tree(X, y, depth, max_depth=None):\n",
    "    if depth == max_depth or len(np.unique(y)) == 1:\n",
    "        leaf = Node(depth, max_depth)\n",
    "        leaf.value = np.mean(y)\n",
    "        return leaf\n",
    "\n",
    "    feature_index, threshold = find_best_split(X, y)\n",
    "\n",
    "    if feature_index is None:\n",
    "        leaf = Node(depth, max_depth)\n",
    "        leaf.value = np.mean(y)\n",
    "        return leaf\n",
    "\n",
    "    node = Node(depth, max_depth)\n",
    "    node.feature_index = feature_index\n",
    "    node.threshold = threshold\n",
    "\n",
    "    X_left, X_right, y_left, y_right = split_data(X, y, feature_index, threshold)\n",
    "\n",
    "    node.left = build_tree(X_left, y_left, depth + 1, max_depth)\n",
    "    node.right = build_tree(X_right, y_right, depth + 1, max_depth)\n",
    "\n",
    "    return node\n",
    "\n",
    "\n",
    "# Make predictions for a single data point\n",
    "def predict_sample(tree, x):\n",
    "    if tree.left is None and tree.right is None:\n",
    "        return tree.value\n",
    "\n",
    "    if x[tree.feature_index] <= tree.threshold:\n",
    "        return predict_sample(tree.left, x)\n",
    "    else:\n",
    "        return predict_sample(tree.right, x)\n",
    "\n",
    "# Make predictions for a set of data points\n",
    "def predict(tree, X):\n",
    "    return np.array([predict_sample(tree, x) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a9a9a-e1f7-4122-b1f9-92ab6636b14c",
   "metadata": {},
   "source": [
    "# Check Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d0672",
   "metadata": {},
   "source": [
    "### Unit test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2be6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f66feb90",
   "metadata": {},
   "source": [
    "### Model Comparison with the Gradient Boosting Classifier from Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878bb373-b599-44e0-ab69-0952c6d36303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6553"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a scikit-learn GradientBoostingClassifier as an reference\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=6, learning_rate=1.0,\n",
    "    max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for y_data\n",
    "y_train[y_train==-1] = 0\n",
    "y_test[y_test==-1] = 0\n",
    "\n",
    "clf_compare = GBDTC(n_estimators=5, learning_rate=1.0, max_depth=1)\n",
    "clf_compare.train(X_train,y_train)\n",
    "clf_compare.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1302c7-a9de-449e-975a-5f63d97c2b29",
   "metadata": {},
   "source": [
    "### Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36715f87-433a-4000-ad68-1bb3f3b251c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1ada1d559a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    " \n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    " \n",
    "# Divide the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Create a CART template\n",
    "tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    " \n",
    "#Train the model on the training data\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions on test data\n",
    "y_pred = tree_classifier.predict(X_test)\n",
    " \n",
    "#Calculate the accuracy of the model<code>\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model accuracy:\", accuracy)\n",
    " \n",
    "#View the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=tree_classifier.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=tree_classifier.classes_)\n",
    "disp.plot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2060",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
