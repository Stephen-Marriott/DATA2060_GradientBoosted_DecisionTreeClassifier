{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0110afd1-1df7-4137-a405-c29bf6278e6b",
   "metadata": {},
   "source": [
    "# Overview of the Gradient Boosted Tree Classifier\n",
    "\n",
    "Gradient Boosting is an ensemble learning method that takes many weak learners (with accuracy slightly above that of a random guess) and combines them sequentially to create a strong learner (model accuracy of >95%).\n",
    "\n",
    "Advantages:\n",
    "- Highly interpretable - the steps are intuitive\n",
    "- Versatile - can be used for both classification and regression problems\n",
    "\n",
    "Disadvantages:\n",
    "- If the hyperparameters aren't tuned carefully, it is easy for the model to overfit the training data\n",
    "- The model can become computationally expensive, as it requires subsequently training multiple weak learners\n",
    "\n",
    "### Representation\n",
    "\n",
    "We have chosen to implement Gradient Boosting for a classification problem, using a shallow decision tree as our weak learner. The final model $F(x)$ is therefore built from a sequence of $N$ trees, where each successive tree corrects the predictions from the previous iteration:\n",
    "\n",
    "$F_{N}(x) = F_{0}(x) + \\eta \\sum_{i=1}^{N} h_{i}(x)$ \n",
    "\n",
    "Where:\n",
    "- $F_{0}(x)$ is the initial prediction\n",
    "- $\\eta$ is the learning rate (a value between 0 and 1)\n",
    "- $h_{i}(x)$ is the prediction made by decision tree $i$, trained on the residuals from the previous trees\n",
    "\n",
    "We are not predicting class labels, but are instead predicting pseudo-residuals that will be used to correct the initial prediction.\n",
    "\n",
    "### Loss\n",
    "\n",
    "As this is a binary classification problem we will be using the Cross-Entropy loss/Log loss as our loss function. The function is:\n",
    "\n",
    "$L(y, \\hat{p}(x)) = -(y \\cdot log(\\hat{p}(x)) + (1-y) \\cdot log(1-\\hat{p}(x)))$\n",
    "\n",
    "Where:\n",
    "- $y$ are the true labels, 1 or 0\n",
    "- $\\hat{p}(x)$ is the probability predictor for the positive class, 1\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "The optimizer is gradient descent on the pseudo-residuals\n",
    "\n",
    "We calculate the pseudo-residuals as the negative gradient of the loss function with respect to the current model prediction:\n",
    "\n",
    "$r_i = -\\frac{\\partial L(y, \\hat{p}(x))}{\\partial F(x)} = y - \\hat{p}(x)$\n",
    "\n",
    "Shallow decision trees are trained on the previous iteration residuals, which are then used to update the prediction\n",
    "\n",
    "$F_{i+1}(x) = F_{i}(x) + \\eta \\cdot h_{i}(x)$ \n",
    "\n",
    "### Algorithm Pseudocode\n",
    "\n",
    "**inputs:**\n",
    "\n",
    "*training set:* $S = (x_1, y_1), ... , (x_m, y_m)$\n",
    "\n",
    "*weak learner: decision tree classifier* $DTC$\n",
    "\n",
    "*number of trees:* $N$\n",
    "\n",
    "*learning rate:* $\\eta$\n",
    "\n",
    "**initialize:**\n",
    "\n",
    "*set initial predictions as log-odds of the positive class:* $F_{0}(x) = logit(p_{y=1}) = log(\\frac{p_{y=1}}{1-p_{y=1}})$ \n",
    "\n",
    "**for** $i = 0, ...,N-1:$\n",
    "\n",
    "*compute the residuals:* $r_i = -\\frac{\\partial L(y, \\hat{p}_{i}(x))}{\\partial F_{i}(x)} = y - \\hat{p}_{i}(x)$\n",
    "\n",
    "*train a weak learner with residuals as targets:* $h_{i}(x) = DTC(F_{i}(x), S)$\n",
    "\n",
    "*update the model:* $F_{i+1}(x) = F_{i}(x) + \\eta \\cdot h_{i}(x)$\n",
    "\n",
    "**output:** \n",
    "\n",
    "*the predictions* $\\hat{y} = argmax(F_{N}(x))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a2637-8438-445a-8925-359496904aa3",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa765c1-b06d-4a46-85fa-61fe2cfeac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def cal_mse(y):\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    \n",
    "    mean = np.mean(y)\n",
    "\n",
    "    return np.mean((y - mean) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c370a-1abc-445c-849c-6fa76015e3d0",
   "metadata": {},
   "source": [
    "### Weak Learner: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57cfae-68ef-47aa-b535-44b567bfcf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, left=None, right=None, depth=0, index_split_on=0, isleaf=False, threshold=None, value=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.depth = depth\n",
    "        self.index_split_on = index_split_on\n",
    "        self.isleaf = isleaf\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.value = value\n",
    "        self.leaf_id = None\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, X, y, max_depth=40, min_samples_split=2, min_samples_leaf=1):\n",
    "        \n",
    "        self.average_value = np.mean(y)\n",
    "        self.max_depth = max_depth\n",
    "        self.root = Node(value=self.average_value)\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        indices = list(range(1, X.shape[1]+1))\n",
    "        self.leaf_id_counter = 1\n",
    "        self.value = []\n",
    "        self._split_recurs(self.root, X, y, indices)\n",
    "        \n",
    "    def predict(self, X):\n",
    "\n",
    "        predictions = []\n",
    "        for i in range(X.shape[0]):\n",
    "            row = X[i,:]\n",
    "            predictions.append(self._predict_recurs(self.root, row))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict_recurs(self, node, row):\n",
    "\n",
    "        if node.isleaf or node.index_split_on == 0:\n",
    "            return node.value\n",
    "\n",
    "        if row[node.index_split_on-1] <= node.threshold:\n",
    "            return self._predict_recurs(node.left, row)\n",
    "        \n",
    "        else:\n",
    "            return self._predict_recurs(node.right, row)\n",
    "        \n",
    "\n",
    "    def _is_terminal(self, node, X, y, indices):\n",
    "\n",
    "        node.isleaf = False\n",
    "        node.value = np.mean(y)\n",
    "\n",
    "        # Terminate if fewer than `min_samples_split`\n",
    "        if len(y) < self.min_samples_split:\n",
    "            node.isleaf = True\n",
    "\n",
    "        # Terminate if fewer than `min_samples_leaf` would remain in a split\n",
    "        if len(y) > 0 and len(y) <= self.min_samples_leaf:\n",
    "            node.isleaf = True\n",
    "\n",
    "        if len(indices) == 0:\n",
    "            node.isleaf = True\n",
    "        \n",
    "        if len(set(y)) == 1:\n",
    "            node.isleaf = True\n",
    "\n",
    "        if node.depth == self.max_depth:\n",
    "            node.isleaf = True\n",
    "\n",
    "        if node.isleaf == True:\n",
    "            node.leaf_id = self.leaf_id_counter\n",
    "            self.leaf_id_counter += 1\n",
    "            self.value.append(node.value)\n",
    "\n",
    "        return node.isleaf, node.value\n",
    "\n",
    "    def find_threshold(self, X, y, mse, indices):\n",
    "        # find the best index and threshold value for the selected index\n",
    "        best_mse = mse\n",
    "        best_idx = None\n",
    "        best_thresh = None\n",
    "\n",
    "        for idx in indices:\n",
    "            #thresholds = np.unique(X[:, idx-1])\n",
    "            #thresholds = np.linspace(min(X[:,idx-1])/2, max(X[:,idx-1])*2, 10000)\n",
    "            sorted_X = np.sort(X[:,idx-1])\n",
    "            thresholds = (sorted_X[:-1] + sorted_X[1:])/2\n",
    "            for threshold in thresholds:\n",
    "                \n",
    "                left_idx = X[:, idx-1] <= threshold\n",
    "                right_idx = X[:, idx-1] > threshold\n",
    "\n",
    "                # Check `min_samples_leaf` condition\n",
    "                if sum(left_idx) < self.min_samples_leaf or sum(right_idx) < self.min_samples_leaf:\n",
    "                    mse = float('inf')\n",
    "                else:\n",
    "                    mse = cal_mse(y[left_idx])+cal_mse(y[right_idx])\n",
    "\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_idx = idx\n",
    "                    best_thresh = threshold\n",
    "\n",
    "        return best_idx, best_thresh\n",
    "\n",
    "    def _split_recurs(self, node, X, y, indices):\n",
    "\n",
    "        node.isleaf, node.value = self._is_terminal(node, X, y, indices)\n",
    "\n",
    "        mse = cal_mse(y)\n",
    "\n",
    "        if not node.isleaf:\n",
    "                \n",
    "            best_idx, best_thresh = self.find_threshold(X, y, mse, indices)\n",
    "            \n",
    "            if best_idx == None:\n",
    "                node.isleaf = True\n",
    "                node.value = np.mean(y)\n",
    "                return\n",
    "\n",
    "            node.threshold = best_thresh\n",
    "            node.index_split_on = best_idx\n",
    "\n",
    "            left_idx = X[:, best_idx-1] <= best_thresh\n",
    "            right_idx = X[:, best_idx-1] > best_thresh\n",
    "\n",
    "            left_X, left_y = X[left_idx], y[left_idx]\n",
    "            right_X, right_y = X[right_idx], y[right_idx]\n",
    "\n",
    "            left_child = Node(depth=node.depth + 1)\n",
    "            right_child = Node(depth=node.depth + 1)\n",
    "            \n",
    "            node.left = left_child\n",
    "            node.right = right_child\n",
    "            \n",
    "            self._split_recurs(left_child, left_X, left_y, indices)\n",
    "            self._split_recurs(right_child, right_X, right_y, indices)\n",
    "\n",
    "    \n",
    "    def apply(self, X):\n",
    "        leaf_indices = []\n",
    "        for i in range(X.shape[0]):\n",
    "            row = X[i,:]\n",
    "            leaf_indices.append(self._apply_recurs(self.root, row))\n",
    "        return np.array(leaf_indices)\n",
    "\n",
    "    def _apply_recurs(self, node, row):\n",
    "        if node.isleaf:\n",
    "            return node.leaf_id  # Return the unique leaf ID\n",
    "        \n",
    "        if row[node.index_split_on - 1] <= node.threshold:\n",
    "            return self._apply_recurs(node.left, row)\n",
    "        else:\n",
    "            return self._apply_recurs(node.right, row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab23c5",
   "metadata": {},
   "source": [
    "### Weak Learner: Decision Tree (Scikit-Learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e291b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() #+ np.random.rand(80) * 0.1\n",
    "\n",
    "# Make predictions\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_test = np.sin(X_test).ravel() #+ np.random.rand(500) * 0.1\n",
    "\n",
    "# Create a decision tree regressor\n",
    "regressor = DecisionTreeRegressor(max_depth=4, random_state=0)  # You can adjust the max_depth parameter\n",
    "\n",
    "regressor.fit(X, y)\n",
    "y_pred1 = regressor.predict(X_test)\n",
    "\n",
    "reg = DecisionTree(X, y, max_depth=4)\n",
    "y_pred2 = reg.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "plt.plot(X_test, y_pred1, color=\"red\", label=\"scikit-learn\", linewidth=1)\n",
    "plt.plot(X_test, y_pred2, color=\"cornflowerblue\", label=\"current\", linewidth=1)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(mean_squared_error(y_test, y_pred1))\n",
    "print(mean_squared_error(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfcf767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "\n",
    "\n",
    "# Decision Tree Regressor Class\n",
    "class RegressionTree:\n",
    "    def __init__(self, n_feats = None, max_depth = 100, min_samples_split = 2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None ,\n",
    "                 random_state=None ,max_leaf_nodes=None, min_impurity_decrease=0.0, ccp_alpha=0.0):\n",
    "        \n",
    "        self.root = None\n",
    "        self.n_feats = n_feats\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
    "        self.col = list(X.columns)\n",
    "        self.root = self.growTree(X, Y)\n",
    "\n",
    "    def growTree(self, X, Y, depth = 0):\n",
    "        \n",
    "        df = X.copy()\n",
    "        df['y'] = Y\n",
    "        \n",
    "        ymean = np.mean(Y)\n",
    "        \n",
    "        self.mse = self.get_mse(Y, ymean)\n",
    "        \n",
    "        n_sample, n_feature = X.shape\n",
    "        \n",
    "        # stopping criteria\n",
    "        if (depth >= self.max_depth or n_sample <= self.min_samples_split):\n",
    "            leaf_value = np.mean(Y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        feats_idxs = list(X.columns)\n",
    "\n",
    "        best_feat, best_thresh = self.best_criteria(X, Y, feats_idxs)\n",
    "\n",
    "        left_df, right_df = df[df[best_feat]<=best_thresh].copy(), df[df[best_feat]>best_thresh].copy()\n",
    "\n",
    "        left = self.growTree(left_df.drop('y', axis=1), left_df['y'].values.tolist(), depth+1)\n",
    "        right = self.growTree(right_df.drop('y', axis=1), right_df['y'].values.tolist(), depth+1)\n",
    "\n",
    "        return Node(best_feat, best_thresh, left, right)\n",
    "    \n",
    "    \n",
    "    # find out best criteria\n",
    "    def best_criteria(self, X, Y, feats_idxs):\n",
    "        \n",
    "        df = X.copy()\n",
    "        \n",
    "        df['y'] = Y\n",
    "        \n",
    "        mse_base = self.mse\n",
    "        \n",
    "        best_feature = None\n",
    "        best_thresh = None\n",
    "        \n",
    "        for feat in feats_idxs:\n",
    "            \n",
    "            xdf = df.sort_values(feat)\n",
    "            \n",
    "            x_mean = self.moving_average(xdf[feat], 2)\n",
    "\n",
    "            for value in x_mean:\n",
    "                left_y = xdf[xdf[feat] < value]['y'].values\n",
    "                right_y = xdf[xdf[feat] >= value]['y'].values\n",
    "                \n",
    "                left_mean = 0\n",
    "                right_mean = 0\n",
    "                if len(left_y) > 0:\n",
    "                    left_mean = np.mean(left_y)\n",
    "                if len(right_y) > 0:\n",
    "                    right_mean = np.mean(right_y)\n",
    "                \n",
    "                res_left = left_y - left_mean\n",
    "                res_right = right_y - right_mean\n",
    "                \n",
    "                r = np.concatenate((res_left, res_right), axis=None)\n",
    "                \n",
    "                n = len(r)\n",
    "\n",
    "                r = r ** 2\n",
    "                r = np.sum(r)\n",
    "                mse_split = r / n\n",
    "                \n",
    "                if mse_split < mse_base:\n",
    "                    mse_base = mse_split\n",
    "                    best_feature = feat\n",
    "                    best_thresh = value\n",
    "                    \n",
    "        return (best_feature, best_thresh)\n",
    "    \n",
    "    def get_mse(self, y_true, y_hat):\n",
    "        n = len(y_true)\n",
    "        \n",
    "        r = y_true - y_hat\n",
    "        \n",
    "        r = r ** 2\n",
    "        \n",
    "        r = np.sum(r)\n",
    "        \n",
    "        return r / n\n",
    "    \n",
    "    def moving_average(self, x:np.array, window : int):\n",
    "        return np.convolve(x, np.ones(window), 'valid') / window \n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = X.to_numpy().tolist()\n",
    "        \n",
    "        return np.array([self.traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def traverse_tree(self, x, node):\n",
    "       \n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        fr = node.feature\n",
    "        index = self.col.index(fr)\n",
    "\n",
    "        if x[index] <= node.threshold:\n",
    "            return self.traverse_tree(x, node.left)\n",
    "        \n",
    "        return self.traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec365e2-4574-450a-ae53-27d493847b7b",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884883fe-0a0a-48f0-ae38-958a71b51b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDTC:\n",
    "    def __init__(self, n_estimators=2, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.init_val = None\n",
    "        self.trees = None\n",
    "        self.gamma_value = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # X: 2D numpy array for input variables\n",
    "        # y: 1D numpy array for output variables\n",
    "        prob = np.zeros([self.n_estimators+1,X.shape[0]])\n",
    "        gamma = np.zeros([self.n_estimators,X.shape[0]])\n",
    "        self.gamma_value = np.zeros([self.n_estimators,X.shape[0]])\n",
    "        F = np.zeros((self.n_estimators+1,X.shape[0]))\n",
    "\n",
    "        self.init_val = np.log(sum(y==1)/(X.shape[0]-sum(y==1)))\n",
    "\n",
    "        prob[0,:] = np.full(X.shape[0], self.init_val)\n",
    "        F[0,:] = np.full(X.shape[0], np.exp(self.init_val)/(1+np.exp(self.init_val)))\n",
    "\n",
    "        self.residuals = np.zeros((self.n_estimators,X.shape[0]))\n",
    "        self.trees = []\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "\n",
    "            # calculate the residual\n",
    "            self.residuals[i] = y-F[i,:]\n",
    "\n",
    "            # tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            # tree.fit(X, self.residuals[i])\n",
    "            \n",
    "            # self.trees.append(tree)\n",
    "            # tree = tree.tree_\n",
    "            # leaf_indices=tree.apply(X) # total leaves in a tree [1 2 2 2 2 1]\n",
    "\n",
    "            tree = DecisionTree(X, self.residuals[i], max_depth=self.max_depth)\n",
    "            self.trees.append(tree)\n",
    "            leaf_indices = tree.apply(X)\n",
    "\n",
    "            unique_leaves=np.unique(leaf_indices) # [1 2]\n",
    "\n",
    "            for idx in unique_leaves:\n",
    "                n_leaf = len(leaf_indices[leaf_indices==idx])\n",
    "                p_old = F[i][leaf_indices==idx]\n",
    "                denominator = np.sum(p_old*(1-p_old))\n",
    "                #self.gamma_value[i,idx] = tree.value[idx][0][0]*n_leaf / denominator\n",
    "                self.gamma_value[i,idx] = tree.value[idx-1]*n_leaf / denominator\n",
    "\n",
    "            gamma[i] = [self.gamma_value[i][index] for index in leaf_indices]\n",
    "\n",
    "            prob[i+1,:] =  prob[i] + self.learning_rate * gamma[i]\n",
    "            F[i+1,:] = np.array([np.exp(prob_elem)/(np.exp(prob_elem)+1) for prob_elem in prob[i+1,:]])\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        prob = np.zeros([self.n_estimators+1,X.shape[0]])\n",
    "        gamma = np.zeros([self.n_estimators,X.shape[0]])\n",
    "        F = np.zeros((self.n_estimators+1,X.shape[0]))\n",
    "\n",
    "        prob[0,:] = np.full(X.shape[0], self.init_val)\n",
    "        F[0,:] = np.full(X.shape[0], np.exp(self.init_val)/(1+np.exp(self.init_val)))\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "\n",
    "            tree = self.trees[i]\n",
    "            leaf_indices = tree.apply(X)\n",
    "\n",
    "            gamma[i] = [self.gamma_value[i,index] for index in leaf_indices]\n",
    "\n",
    "            prob[i+1,:] =  prob[i] + self.learning_rate * gamma[i]\n",
    "            F[i+1,:] = np.array([np.exp(prob_elem)/(np.exp(prob_elem)+1) for prob_elem in prob[i+1,:]])\n",
    "\n",
    "        predictions = (F[self.n_estimators]>0.5) * 1.0\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a9a9a-e1f7-4122-b1f9-92ab6636b14c",
   "metadata": {},
   "source": [
    "# Check Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d0672",
   "metadata": {},
   "source": [
    "### Unit test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66feb90",
   "metadata": {},
   "source": [
    "### Model Comparison with the Gradient Boosting Classifier from Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878bb373-b599-44e0-ab69-0952c6d36303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "X_train, X_test = X[:2000], X[2000:,:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=4, learning_rate=0.1,\n",
    "    max_depth=2).fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = clf.predict(X_test)\n",
    "accuracy1 = np.sum(y_test==y_pred1)/len(y_test)\n",
    "\n",
    "y_train[y_train==-1] = 0\n",
    "y_test[y_test==-1] = 0\n",
    "\n",
    "model2 = GBDTC(n_estimators=4,\n",
    "              learning_rate=0.1,\n",
    "              max_depth=2)\n",
    "\n",
    "model2.fit(X_train,y_train)\n",
    "y_pred2 = model2.predict(X_test)\n",
    "accuracy2 = np.sum(y_test==y_pred2)/len(y_test)\n",
    "\n",
    "\n",
    "print(f'accuracy1={accuracy1}')\n",
    "print(f'accuracy2={accuracy2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5566cd-8900-4ffb-9eb4-3e02fef5d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "#from GBDTC import cal_mse, DecisionTree\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def test_case_1():\n",
    "    \"\"\"\n",
    "    Test Gradient Descent using numeric inputs and fixed gradients.\n",
    "    \"\"\"\n",
    "    x_values = np.array([5.0, 3.0, -1.0])\n",
    "    gradients = np.array([10.0, 6.0, -2.0])  \n",
    "\n",
    "    gd = GradientDescent(learning_rate=0.1, max_iter=3, tolerance=1e-6)\n",
    "    updated_values = []\n",
    "\n",
    "    for x, grad in zip(x_values, gradients):\n",
    "        updated_value = x - gd.learning_rate * grad\n",
    "        updated_values.append(updated_value)\n",
    "\n",
    "    expected_values = np.array([4.0, 2.4, -0.8])  \n",
    "    assert np.allclose(updated_values, expected_values, atol=1e-3), f\"Expected {expected_values}, got {updated_values}\"\n",
    "\n",
    "\n",
    "def test_case_2():\n",
    "    \"\"\"\n",
    "    Test Gradient Descent on a small set of numeric data points to ensure convergence.\n",
    "    \"\"\"\n",
    "    data_points = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "    gradients = np.array([0.5, 1.0, 1.5, 2.0])\n",
    "\n",
    "    gd = GradientDescent(learning_rate=0.2, max_iter=1, tolerance=1e-6)\n",
    "    updated_points = data_points - gd.learning_rate * gradients\n",
    "\n",
    "    expected_points = np.array([0.9, 1.8, 2.7, 3.6])\n",
    "    assert np.allclose(updated_points, expected_points, atol=1e-3), f\"Expected {expected_points}, got {updated_points}\"\n",
    "\n",
    "\n",
    "def test_case_3():\n",
    "    \"\"\"\n",
    "    Test Gradient Descent with zero gradients to ensure no update occurs.\n",
    "    \"\"\"\n",
    "    x_values = np.array([4.0, -3.0, 7.0])\n",
    "    gradients = np.array([0.0, 0.0, 0.0])  \n",
    "\n",
    "    gd = GradientDescent(learning_rate=0.1, max_iter=3, tolerance=1e-6)\n",
    "    updated_values = []\n",
    "\n",
    "    for x, grad in zip(x_values, gradients):\n",
    "        updated_value = x - gd.learning_rate * grad\n",
    "        updated_values.append(updated_value)\n",
    "\n",
    "    expected_values = np.array([4.0, -3.0, 7.0])  # Should remain unchanged\n",
    "    assert np.allclose(updated_values, expected_values, atol=1e-3), f\"Expected {expected_values}, got {updated_values}\"\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def test_cases():\n",
    "    \"\"\"\n",
    "    Executes all Gradient Descent test cases.\n",
    "    \"\"\"\n",
    "    test_case_1()\n",
    "    test_case_2()\n",
    "    test_case_3()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0a8e8-413b-4fdc-ac63-d36e6029b96c",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6318522d-520d-43cc-bfd5-35188ef234eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Reads input data, initializes Gradient Descent, and prints the results.\n",
    "    \"\"\"\n",
    "    # Example Input Data\n",
    "    x_data = np.array([5.0, 10.0, -2.0, 8.0])\n",
    "    gradient_data = np.array([1.0, 2.0, -0.5, 1.5])\n",
    "\n",
    "    gd = GradientDescent(learning_rate=0.1, max_iter=10, tolerance=1e-6)\n",
    "\n",
    "    print(\"Initial Data:\")\n",
    "    print(f\"X Values: {x_data}\")\n",
    "    print(f\"Gradients: {gradient_data}\")\n",
    "\n",
    "    print(\"\\nRunning Gradient Descent Updates:\")\n",
    "    updated_values = []\n",
    "\n",
    "    for x, grad in zip(x_data, gradient_data):\n",
    "        updated_value = x - gd.learning_rate * grad\n",
    "        updated_values.append(updated_value)\n",
    "        print(f\"Value: {x} -> Updated: {updated_value} (Gradient: {grad})\")\n",
    "\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Updated Values: {updated_values}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pytest.main([\"-v\"])  \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1302c7-a9de-449e-975a-5f63d97c2b29",
   "metadata": {},
   "source": [
    "### Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36715f87-433a-4000-ad68-1bb3f3b251c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    " \n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    " \n",
    "# Divide the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Create a CART template\n",
    "tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    " \n",
    "#Train the model on the training data\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions on test data\n",
    "y_pred = tree_classifier.predict(X_test)\n",
    " \n",
    "#Calculate the accuracy of the model<code>\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model accuracy:\", accuracy)\n",
    " \n",
    "#View the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=tree_classifier.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=tree_classifier.classes_)\n",
    "disp.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee5ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
