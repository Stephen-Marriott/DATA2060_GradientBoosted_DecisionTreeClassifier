{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0110afd1-1df7-4137-a405-c29bf6278e6b",
   "metadata": {},
   "source": [
    "# Overview of the Gradient Boosted Tree Classifier\n",
    "\n",
    "Gradient Boosting is an ensemble learning method that takes many weak learners (with accuracy slightly above that of a random guess) and combines them sequentially to create a strong learner (model accuracy of >95%).\n",
    "\n",
    "Advantages:\n",
    "- Highly interpretable - the steps are intuitive\n",
    "- Versatile - can be used for both classification and regression problems\n",
    "\n",
    "Disadvantages:\n",
    "- If the hyperparameters aren't tuned carefully, it is easy for the model to overfit the training data\n",
    "- The model can become computationally expensive, as it requires subsequently training multiple weak learners\n",
    "\n",
    "### Representation\n",
    "\n",
    "We have chosen to implement Gradient Boosting for a classification problem, using a shallow decision tree as our weak learner. The final model $F(x)$ is therefore built from a sequence of $N$ trees, where each successive tree corrects the predictions from the previous iteration:\n",
    "\n",
    "$F_{N}(x) = F_{0}(x) + \\eta \\sum_{i=1}^{N} h_{i}(x)$ \n",
    "\n",
    "Where:\n",
    "- $F_{0}(x)$ is the initial prediction\n",
    "- $\\eta$ is the learning rate (a value between 0 and 1)\n",
    "- $h_{i}(x)$ is the prediction made by decision tree $i$, trained on the residuals from the previous trees\n",
    "\n",
    "We are not predicting class labels, but are instead predicting pseudo-residuals that will be used to correct the initial prediction.\n",
    "\n",
    "### Loss\n",
    "\n",
    "As this is a binary classification problem we will be using the Cross-Entropy loss/Log loss as our loss function. The function is:\n",
    "\n",
    "$L(y, \\hat{p}(x)) = -(y \\cdot log(\\hat{p}(x)) + (1-y) \\cdot log(1-\\hat{p}(x)))$\n",
    "\n",
    "Where:\n",
    "- $y$ are the true labels, 1 or 0\n",
    "- $\\hat{p}(x)$ is the probability predictor for the positive class, 1\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "The optimizer is gradient descent on the pseudo-residuals\n",
    "\n",
    "We calculate the pseudo-residuals as the negative gradient of the loss function with respect to the current model prediction:\n",
    "\n",
    "$r_i = -\\frac{\\partial L(y, \\hat{p}(x))}{\\partial F(x)} = y - \\hat{p}(x)$\n",
    "\n",
    "Shallow decision trees are trained on the previous iteration residuals, which are then used to update the prediction\n",
    "\n",
    "$F_{i+1}(x) = F_{i}(x) + \\eta \\cdot h_{i}(x)$ \n",
    "\n",
    "### Algorithm Pseudocode\n",
    "\n",
    "**inputs:**\n",
    "\n",
    "&emsp; *training set:* $S = (x_1, y_1), ... , (x_m, y_m)$\n",
    "\n",
    "&emsp; *weak learner: decision tree classifier* $DTC$\n",
    "\n",
    "&emsp; *number of trees:* $N$\n",
    "\n",
    "&emsp; *learning rate:* $\\eta$\n",
    "\n",
    "**initialize:**\n",
    "\n",
    "&emsp; *set initial predictions as log-odds of the positive class:* $F_{0}(x) = logit(p_{y=1}) = log(\\frac{p_{y=1}}{1-p_{y=1}})$ \n",
    "\n",
    "&emsp; **for** $i = 0, ...,N-1:$\n",
    "\n",
    "&emsp;&emsp; *compute the residuals:* $r_i = -\\frac{\\partial L(y, \\hat{p}_{i}(x))}{\\partial F_{i}(x)} = y - \\hat{p}_{i}(x)$\n",
    "\n",
    "&emsp;&emsp; *train a weak learner with residuals as targets:* $h_{i}(x) = DTC(F_{i}(x), S)$\n",
    "\n",
    "&emsp;&emsp; *update the model:* $F_{i+1}(x) = F_{i}(x) + \\eta \\cdot h_{i}(x)$\n",
    "\n",
    "**output:** \n",
    "\n",
    "&emsp; *the predictions* $\\hat{y} = argmax(F_{N}(x))$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a2637-8438-445a-8925-359496904aa3",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c370a-1abc-445c-849c-6fa76015e3d0",
   "metadata": {},
   "source": [
    "### Weak Learner: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a57cfae-68ef-47aa-b535-44b567bfcf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "\n",
    "def node_score_error(prob):\n",
    "    '''\n",
    "        TODO:\n",
    "        Calculate the node score using the train error of the subdataset and return it.\n",
    "        For a dataset with two classes, C(p) = min{p, 1-p}\n",
    "    '''\n",
    "\n",
    "    return min([prob, 1-prob])\n",
    "    \n",
    "    pass\n",
    "\n",
    "def node_score_entropy(prob):\n",
    "    '''\n",
    "        TODO:\n",
    "        Calculate the node score using the entropy of the subdataset and return it.\n",
    "        For a dataset with 2 classes, C(p) = -p * log(p) - (1-p) * log(1-p)\n",
    "        For the purposes of this calculation, assume 0*log0 = 0.\n",
    "        HINT: remember to consider the range of values that p can take!\n",
    "    '''\n",
    "    # HINT: If p < 0 or p > 1 then entropy = 0\n",
    "\n",
    "    if prob == 0 or prob == 1:\n",
    "\n",
    "        entropy = 0\n",
    "\n",
    "    else:\n",
    "\n",
    "        entropy = -(prob*np.log(prob)) - ((1-prob)*np.log(1-prob))\n",
    "    \n",
    "    return entropy\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def node_score_gini(prob):\n",
    "    '''\n",
    "        TODO:\n",
    "        Calculate the node score using the gini index of the subdataset and return it.\n",
    "        For dataset with 2 classes, C(p) = 2 * p * (1-p)\n",
    "    '''\n",
    "\n",
    "    return (2 * prob * (1 - prob))\n",
    "    \n",
    "    pass\n",
    "\n",
    "class Node:\n",
    "    '''\n",
    "    Helper to construct the tree structure.\n",
    "    '''\n",
    "    def __init__(self, left=None, right=None, depth=0, index_split_on=0, isleaf=False, label=1):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.depth = depth\n",
    "        self.index_split_on = index_split_on\n",
    "        self.isleaf = isleaf\n",
    "        self.label = label\n",
    "        self.info = {} # used for visualization\n",
    "\n",
    "\n",
    "    def _set_info(self, gain, num_samples):\n",
    "        '''\n",
    "        Helper function to add to info attribute.\n",
    "        You do not need to modify this. \n",
    "        '''\n",
    "\n",
    "        self.info['gain'] = gain\n",
    "        self.info['num_samples'] = num_samples\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, data, validation_data=None, gain_function=node_score_entropy, max_depth=40):\n",
    "\n",
    "        # TODO: find majority class; set to class 0 if exactly balanced. This is the default label of your root node.\n",
    "        # Make sure to assign it to an instance variable `majority_class`.\n",
    "\n",
    "        y = [row[0] for row in data]\n",
    "        count_0 = y.count(0)\n",
    "        count_1 = y.count(1)\n",
    "        \n",
    "        if count_0 >= count_1:\n",
    "            self.majority_class = 0 \n",
    "        else:\n",
    "            self.majority_class = 1\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.root = Node(label = self.majority_class)\n",
    "        self.gain_function = gain_function\n",
    "\n",
    "        indices = list(range(1, len(data[0])))\n",
    "\n",
    "        self._split_recurs(self.root, data, indices)\n",
    "\n",
    "        # Pruning\n",
    "        if validation_data is not None:\n",
    "            self._prune_recurs(self.root, validation_data)\n",
    "\n",
    "\n",
    "    def predict(self, features):\n",
    "        '''\n",
    "        Helper function to predict the label given a row of features.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        return self._predict_recurs(self.root, features)\n",
    "\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        '''\n",
    "        Helper function to calculate the accuracy on the given data.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        return 1 - self.loss(data)\n",
    "\n",
    "\n",
    "    def loss(self, data):\n",
    "        '''\n",
    "        Helper function to calculate the loss on the given data.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        cnt = 0.0\n",
    "        test_Y = [row[0] for row in data]\n",
    "        for i in range(len(data)):\n",
    "            prediction = self.predict(data[i])\n",
    "            if (prediction != test_Y[i]):\n",
    "                cnt += 1.0\n",
    "        return cnt/len(data)\n",
    "\n",
    "\n",
    "    def _predict_recurs(self, node, row):\n",
    "        '''\n",
    "        Helper function to predict the label given a row of features.\n",
    "        Traverse the tree until leaves to get the label.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        if node.isleaf or node.index_split_on == 0:\n",
    "            return node.label\n",
    "        split_index = node.index_split_on\n",
    "        if not row[split_index]:\n",
    "            return self._predict_recurs(node.left, row)\n",
    "        else:\n",
    "            return self._predict_recurs(node.right, row)\n",
    "\n",
    "\n",
    "    def _prune_recurs(self, node, validation_data):\n",
    "        '''\n",
    "        TODO:\n",
    "        Prune the tree bottom up recursively. Nothing needs to be returned.\n",
    "        Do not prune if the node is a leaf.\n",
    "        Do not prune if the node is non-leaf and has at least one non-leaf child.\n",
    "        Prune if deleting the node could reduce loss on the validation data.\n",
    "        NOTE:\n",
    "        This might be slightly different from the pruning described in lecture.\n",
    "        Here we won't consider pruning a node's parent if we don't prune the node \n",
    "        itself (i.e. we will only prune nodes that have two leaves as children.)\n",
    "        HINT: Think about what variables need to be set when pruning a node!\n",
    "        '''\n",
    "        # Checks if node is not a Leaf\n",
    "        if node.isleaf:\n",
    "            return\n",
    "        \n",
    "        if not node.isleaf:\n",
    "            if node.left is not None:\n",
    "                # TODO: Prune node.left\n",
    "                self._prune_recurs(node.left, validation_data)\n",
    "                pass\n",
    "            if node.right is not None:\n",
    "                # TODO: Prune node.right\n",
    "                self._prune_recurs(node.right, validation_data)\n",
    "                pass\n",
    "            if (node.left.isleaf) and (node.right.isleaf):\n",
    "                # TODO: Prune node only if loss is reduced\n",
    "                current_loss = self.loss(validation_data)\n",
    "\n",
    "                if node.left.label == 1:\n",
    "                    majority = 1 \n",
    "                else: \n",
    "                    majority = 0\n",
    "\n",
    "                node.isleaf = True\n",
    "                node.label = majority\n",
    "                \n",
    "                new_loss = self.loss(validation_data)\n",
    "                \n",
    "                if new_loss < current_loss:\n",
    "                    node.left = None\n",
    "                    node.right = None\n",
    "                    \n",
    "                else:\n",
    "                    node.isleaf = False\n",
    "                    \n",
    "                    if node.left.label == node.right.label:\n",
    "                        node.label = majority  \n",
    "                    else:\n",
    "                        None\n",
    "                        \n",
    "                pass\n",
    "                    \n",
    "        \n",
    "    def _is_terminal(self, node, data, indices):\n",
    "        '''\n",
    "        TODO:\n",
    "        Helper function to determine whether the node should stop splitting.\n",
    "        Stop the recursion if:\n",
    "            1. The dataset (as passed to parameter data) is empty.\n",
    "            2. There are no more indices to split on.\n",
    "            3. All the instances in this dataset belong to the same class\n",
    "            4. The depth of the node reaches the maximum depth.\n",
    "        Set the node label to be the majority label of the training dataset if:\n",
    "            1. The number of class 1 points is equal to the number of class 0 points.\n",
    "            2. The dataset is empty.\n",
    "        Return:\n",
    "            - A boolean, True indicating the current node should be a leaf and \n",
    "              False if the node is not a leaf.\n",
    "            - A label, indicating the label of the leaf (or the label the node would \n",
    "              be if we were to terminate at that node). If there is no data left, you\n",
    "              must return the majority class of the training set.\n",
    "        '''\n",
    "        y = [row[0] for row in data]        \n",
    "        \n",
    "        # TODO: Check Cases if the node should stop splitting\n",
    "        # TODO: Check cases if the node should be set to the majority label of the training dataset\n",
    "        if len(data) == 0:\n",
    "            return True, self.majority_class\n",
    "\n",
    "        elif len(indices) == 0:\n",
    "            if y.count(0) >= y.count(1):\n",
    "                majority = 0\n",
    "            else: \n",
    "                majority = 1         \n",
    "            return True, majority\n",
    "\n",
    "        elif all(label == y[0] for label in y):\n",
    "            return True, y[0]\n",
    "\n",
    "        elif node.depth >= self.max_depth:\n",
    "            if y.count(0) >= y.count(1):\n",
    "                majority = 0\n",
    "            else: \n",
    "                majority = 1               \n",
    "            return True, majority\n",
    "\n",
    "        if y.count(0) >= y.count(1):\n",
    "            majority = 0\n",
    "        else: \n",
    "            majority = 1 \n",
    "        \n",
    "        return False, majority\n",
    "        \n",
    "        pass\n",
    "        \n",
    "\n",
    "    def _split_recurs(self, node, data, indices):\n",
    "        '''\n",
    "        TODO:\n",
    "        Recursively split the node based on the rows and indices given.\n",
    "        Nothing needs to be returned.\n",
    "\n",
    "        First use _is_terminal() to check if the node needs to be split.\n",
    "        If so, select the column that has the maximum infomation gain to split on.\n",
    "        Store the label predicted for this node, the split column, and use _set_info()\n",
    "        to keep track of the gain and the number of datapoints at the split.\n",
    "        Then, split the data based on its value in the selected column.\n",
    "        The data should be recursively passed to the children.\n",
    "        '''\n",
    "        # TODO: Check if node needs to be split\n",
    "\n",
    "        is_terminal, label = self._is_terminal(node, data, indices)\n",
    "\n",
    "        if is_terminal:\n",
    "            node.isleaf = True\n",
    "            node.label = label\n",
    "            return\n",
    "        \n",
    "        if not node.isleaf:\n",
    "            \n",
    "            # TODO: Initialize and Calculate Gain\n",
    "            \n",
    "            best_gain = float('-inf')\n",
    "            best_index = None\n",
    "            best_splits = None\n",
    "            \n",
    "            for i in indices:\n",
    "\n",
    "                gain = self._calc_gain(data, i, self.gain_function)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_index = i\n",
    "        \n",
    "                    left_split = [row for row in data if row[i] == 0]\n",
    "                    right_split = [row for row in data if row[i] == 1]\n",
    "                    best_splits = (left_split, right_split)\n",
    "\n",
    "            if best_index is None:\n",
    "                node.isleaf = True\n",
    "                node.label = label\n",
    "                return\n",
    "                \n",
    "            # TODO: Split the column and use _set_info()\n",
    "            \n",
    "            node.index_split_on = best_index\n",
    "            node._set_info(best_gain, len(data))   \n",
    "\n",
    "            # TODO: Split the Data and Pass it recursively to the  children\n",
    "\n",
    "            remaining_indices = [i for i in indices if i != best_index]\n",
    "        \n",
    "            left_child = Node(depth=node.depth + 1, label=self.majority_class)\n",
    "            right_child = Node(depth=node.depth + 1, label=self.majority_class)\n",
    "\n",
    "            node.left = left_child\n",
    "            node.right = right_child\n",
    "\n",
    "            self._split_recurs(left_child, best_splits[0], remaining_indices)\n",
    "            self._split_recurs(right_child, best_splits[1], remaining_indices)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def _calc_gain(self, data, split_index, gain_function):\n",
    "        '''\n",
    "        TODO:\n",
    "        Calculate the gain of the proposed splitting and return it.\n",
    "        Gain = C(P[y=1]) - P[x_i=True] * C(P[y=1|x_i=True]) - P[x_i=False] * C(P[y=0|x_i=False])\n",
    "        Here the C(p) is the gain_function. For example, if C(p) = min(p, 1-p), this would be\n",
    "        considering training error gain. Other alternatives are entropy and gini functions.\n",
    "        '''\n",
    "        y = [row[0] for row in data]\n",
    "        xi = [row[split_index] for row in data]\n",
    "\n",
    "        if len(y) != 0 and len(xi) != 0:\n",
    "            # TODO: Calculate \n",
    "\n",
    "            p_y1 = y.count(1)/len(y)\n",
    "\n",
    "            p_xi_true = xi.count(True)/len(xi)\n",
    "\n",
    "            p_xi_false = xi.count(False)/len(xi)\n",
    "\n",
    "            if p_xi_true > 0:\n",
    "                p_y1_given_xi_true = sum(1 for i in range(len(y)) if y[i] == 1 and xi[i]) / xi.count(True)\n",
    "                \n",
    "            else:\n",
    "                p_y1_given_xi_true = 0\n",
    "            \n",
    "            if p_xi_false > 0:\n",
    "                p_y1_given_xi_false = sum(1 for i in range(len(y)) if y[i] == 1 and not xi[i]) / xi.count(False)\n",
    "            \n",
    "            else:           \n",
    "                p_y1_given_xi_false = 0\n",
    "            \n",
    "            gain = (gain_function(p_y1) - p_xi_true * gain_function(p_y1_given_xi_true) - p_xi_false * gain_function(p_y1_given_xi_false))\n",
    "            \n",
    "        else:\n",
    "            gain = 0\n",
    "        return gain\n",
    "    \n",
    "\n",
    "    def print_tree(self):\n",
    "        '''\n",
    "        Helper function for tree_visualization.\n",
    "        Only effective with very shallow trees.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        print('---START PRINT TREE---')\n",
    "        def print_subtree(node, indent=''):\n",
    "            if node is None:\n",
    "                return str(\"None\")\n",
    "            if node.isleaf:\n",
    "                return str(node.label)\n",
    "            else:\n",
    "                decision = 'split attribute = {:d}; gain = {:f}; number of samples = {:d}'.format(node.index_split_on, node.info['gain'], node.info['num_samples'])\n",
    "            left = indent + '0 -> '+ print_subtree(node.left, indent + '\\t\\t')\n",
    "            right = indent + '1 -> '+ print_subtree(node.right, indent + '\\t\\t')\n",
    "            return (decision + '\\n' + left + '\\n' + right)\n",
    "\n",
    "        print(print_subtree(self.root))\n",
    "        print('----END PRINT TREE---')\n",
    "\n",
    "\n",
    "    def loss_plot_vec(self, data):\n",
    "        '''\n",
    "        Helper function to visualize the loss when the tree expands.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        self._loss_plot_recurs(self.root, data, 0)\n",
    "        loss_vec = []\n",
    "        q = [self.root]\n",
    "        num_correct = 0\n",
    "        while len(q) > 0:\n",
    "            node = q.pop(0)\n",
    "            num_correct = num_correct + node.info['curr_num_correct']\n",
    "            loss_vec.append(num_correct)\n",
    "            if node.left != None:\n",
    "                q.append(node.left)\n",
    "            if node.right != None:\n",
    "                q.append(node.right)\n",
    "\n",
    "        return 1 - np.array(loss_vec)/len(data)\n",
    "\n",
    "\n",
    "    def _loss_plot_recurs(self, node, rows, prev_num_correct):\n",
    "        '''\n",
    "        Helper function to visualize the loss when the tree expands.\n",
    "        You do not need to modify this.\n",
    "        '''\n",
    "        labels = [row[0] for row in rows]\n",
    "        curr_num_correct = labels.count(node.label) - prev_num_correct\n",
    "        node.info['curr_num_correct'] = curr_num_correct\n",
    "\n",
    "        if not node.isleaf:\n",
    "            left_data, right_data = [], []\n",
    "            left_num_correct, right_num_correct = 0, 0\n",
    "            for row in rows:\n",
    "                if not row[node.index_split_on]:\n",
    "                    left_data.append(row)\n",
    "                else:\n",
    "                    right_data.append(row)\n",
    "\n",
    "            left_labels = [row[0] for row in left_data]\n",
    "            left_num_correct = left_labels.count(node.label)\n",
    "            right_labels = [row[0] for row in right_data]\n",
    "            right_num_correct = right_labels.count(node.label)\n",
    "\n",
    "            if node.left != None:\n",
    "                self._loss_plot_recurs(node.left, left_data, left_num_correct)\n",
    "            if node.right != None:\n",
    "                self._loss_plot_recurs(node.right, right_data, right_num_correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec365e2-4574-450a-ae53-27d493847b7b",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "884883fe-0a0a-48f0-ae38-958a71b51b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostedClassifier:\n",
    "    def __init__(self, n_trees=10, learning_rate=0.1):\n",
    "        self.n_trees = n_trees\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trees = []\n",
    "        self.initial_prediction = None\n",
    "\n",
    "    def _log_odds(self, y):\n",
    "        p = np.mean(y)\n",
    "        return np.log(p / (1 - p))\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def loss(self, y, preds):\n",
    "        preds = np.clip(preds, 1e-15, 1 - 1e-15)\n",
    "        return -np.mean(y * np.log(preds) + (1 - y) * np.log(1 - preds))\n",
    "\n",
    "    def train(self, X, y):\n",
    "        self.initial_prediction = self._log_odds(y)\n",
    "        F = np.full(y.shape, self.initial_prediction)\n",
    "\n",
    "        #TODO: Fix the below code for using the implemented DecisionTree class, rather than the built-in decision tree\n",
    "        for i in range(self.n_trees):\n",
    "            residuals = y - self._sigmoid(F)\n",
    "            tree = DecisionTree(data=data, gain_function=node_score_gini, max_depth=3) #TODO: DecisionTree class takes X and y zipped.\n",
    "\n",
    "            F += self.learning_rate * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "        return F\n",
    "    \n",
    "    def predict(self, X):\n",
    "        F = np.full((X.shape[0],), self.initial_prediction)\n",
    "        for tree in self.trees:\n",
    "            F += self.learning_rate * tree.predict(X)\n",
    "        return self._sigmoid(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a9a9a-e1f7-4122-b1f9-92ab6636b14c",
   "metadata": {},
   "source": [
    "# Check Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c13fe2-7373-4aca-a33e-39547d085be1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
